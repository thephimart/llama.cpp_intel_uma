# llama.cpp Intel UMA (Core Ultra / ARC iGPU)

This repository provides a Windows-first, Intel-optimized llama.cpp setup for Core Ultra systems using Intel ARC / XPU via SYCL.

**ğŸ¯ Focused testing** was performed on Intel Core Ultra 7 155H, with notes for Core Ultra 200 and 300 series systems.

> ğŸ’¡ **Philosophy**: "This repo exists because Intel UMA deserves real tuning, not copy-pasted CUDA defaults."

| What this repo is | What this repo is not |
|-------------------|----------------------|
| Intel UMAâ€“tuned | CUDA defaults copy-pasted |
| Thermal-aware | Max-fans benchmark chasing |
| Long-context focused | Short prompt demo rigs |

## ğŸ“ Directory Layout

```
C:\llama.cpp
â”‚
â”œâ”€ configs\                      # Configuration files
â”‚   â”œâ”€ ZZZ-Base-*.cfg          # Global base configuration
â”‚   â””â”€ *.cfg                   # Model-specific overrides
â”‚
â”œâ”€ sycl\                        # llama.cpp SYCL build
â”‚   â””â”€ llama.cpp SYCL release
â”‚       â”œâ”€ llama-server.exe
â”‚       â””â”€ llama-cli.exe
â”‚
â”œâ”€ docs\                        # Documentation
â”‚   â””â”€ guides\
â”‚
â”œâ”€ start-llama-server.ps1       # Main launcher script
â”œâ”€ COMPACT.md                   # Context compaction guide
â”œâ”€ llama.cpp model folder.lnk   # Model folder shortcut
â”œâ”€ llama.ico                    # Icon
â””â”€ README.md                    # This file
```

## ğŸš€ Installation

### 1. Clone Repository
Clone this repo directly into `C:\`:
```bash
cd C:\
git clone https://github.com/thephimart/llama.cpp_intel_uma.git llama.cpp
```

### 2. Download llama.cpp SYCL Release
Get the latest release from:
[https://github.com/ggml-org/llama.cpp/releases](https://github.com/ggml-org/llama.cpp/releases)

Extract it to:
```
C:\llama.cpp\sycl
```

### 3. Models
Place GGUF models in:
```
%USERPROFILE%\AppData\Local\llama.cpp
```
> ğŸ’¡ Or let Hugging Face downloads populate it automatically.

## ğŸ–¥ï¸ Starting the Server

Run the interactive launcher:
```powershell
.\start-llama-server.ps1
```

### Creating a Shortcut

**Method 1: Desktop Shortcut**
1. Right-click `start-llama-server.ps1` in File Explorer
2. Select **Send to > Desktop (create shortcut)**
3. Right-click the new shortcut and select **Properties**
4. Click **Change Icon...** and browse to `C:\llama.cpp\llama.ico`
5. Rename the shortcut (e.g., "Llama Server")

**Method 2: Start Menu Pinning**

*Option A - From Desktop:*
1. Create the desktop shortcut using Method 1 above
2. Right-click the shortcut and select **Pin to Start**

*Option B - Direct to Start Menu:*
1. Press `Win + R`, type `shell:Start Menu` and press Enter
2. Navigate to `Programs` folder
3. Right-click and select **New > Shortcut**
4. Browse to `C:\llama.cpp\start-llama-server.ps1`
5. Name it "Llama Server" and click **Finish**
6. Right-click the shortcut > **Properties** > **Change Icon...**
7. Browse to `C:\llama.cpp\llama.ico` and apply

**Method 3: Taskbar Pinning**
1. First create a desktop shortcut using Method 1
2. Right-click the shortcut and select **Pin to taskbar**
3. Or drag the shortcut directly to the taskbar

> ğŸ’¡ **Tip**: The provided `llama.ico` file gives your shortcut a professional appearance in the Start Menu, taskbar, and desktop.

### Configuration Workflow

The launcher automatically sets Intel SYCL environment variables and guides you through:

1. **Backend** - Shows selected backend (SYCL for Intel GPU/XPU)

2. **WebUI Toggle** 
   - Default: Disabled (`--no-webui`)
   - Option [1]: Enable web interface

3. **Network Mode**
   - [1] Local only (127.0.0.1) - Default
   - [2] Shared / LAN (0.0.0.0)

4. **Port Configuration**
   - Default: 11434 (Ollama-compatible)
   - Custom port supported

5. **Model Selection**
   - **[0]** No model (server-only mode)
   - **[H]** Hugging Face repository  
     (e.g., `unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF:Q8_0`)
   - **[1-N]** Local GGUF files from `%USERPROFILE%\AppData\Local\llama.cpp`

6. **Base Config Selection**
   - **[0]** No base config (use llama.cpp defaults)
   - **[1]** ZZZ-Base-Config.cfg (default) - Standard chat/completion config
   - **[2]** ZZZ-Base-Embed-Config.cfg - Embedding-optimized config (no batch sizes)

7. **Runtime Config Override**
   - **[0]** No extra config
   - **[1-N]** Other `*.cfg` files (excluding `ZZZ-Base` files)

8. **Final Configuration Review**
   - Complete summary before launch
   - Option to restart configuration or proceed

### Validation & Safety

- Automatically validates `llama-server.exe` exists in `sycl\` directory
- Verifies `--mmproj` files exist before launching vision models
- Shows complete command line before execution
- Supports retry on configuration errors

## âš™ï¸ Configuration System

### Base Config (Required)

Base configs are auto-detected from:
```
configs\ZZZ-Base-*.cfg
```

Two base configurations are available:

**1. ZZZ-Base-Config.cfg** (default) - Standard configuration for chat/completion models:
- ğŸ§µ **Threading** - 20 threads
- ğŸ“¦ **Batch sizes** - 256 batch / 128 micro-batch
- ğŸ—„ï¸ **KV cache quantization** - q8_0 for both K and V
- âš¡ **Parallelism** - Single parallel request
- ğŸ’¾ **Cache behavior** - Automatic RAM cache (`-1`)

**2. ZZZ-Base-Embed-Config.cfg** - Optimized for embedding models:
- Same as base but without batch size constraints
- Ideal for embedding and retrieval workloads

> â„¹ï¸ Base configs are always applied first before per-model overrides.

### Per-Model Configs (Optional)

After selecting a model, you may apply one additional config file.

These are layered after the base config and are ideal for:
- ğŸ“ **Context size overrides** - Custom context lengths
- ğŸ”§ **Model-specific batch tuning** - Per-model optimization
- ğŸ‘ï¸ **Vision models** - `--mmproj` configurations

## ğŸ”§ Intel SYCL / ARC Notes

The launcher automatically sets these environment variables:
```bash
SYCL_DEVICE_FILTER=level_zero:gpu
SYCL_UR_USE_LEVEL_ZERO_V2=1
```

This enables:
- ğŸš€ **ARC iGPU / XPU acceleration** - Hardware acceleration
- ğŸ’ª **Above 4GB Allocation** - Large model support
- âš¡ **Flash Attention** - Required for KV cache quantization
- ğŸ”„ **Efficient UMA memory sharing** - Unified memory architecture

> âš ï¸ **Important**: `--n-gpu-layers 0` is required in base config to enable flash attention.

## ğŸ“š Context Compaction

See [COMPACT.md](COMPACT.md) for the recommended context compaction prompt, designed for:

- ğŸ’» **Long coding sessions**
- ğŸŒ **Large-context models**
- ğŸ”„ **Iterative refinement workflows**

## ğŸ“„ License

MIT License â€” use it, fork it, break it, improve it.

---

**ğŸ‰ Happy inference!** If you find this useful, consider giving it a â­!
