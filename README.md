# llama.cpp_intel_uma
Configuration guidance for running a variety of models with llama.cpp on Intel UMA
